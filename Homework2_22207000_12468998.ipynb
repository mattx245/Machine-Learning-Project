{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd18450f",
   "metadata": {},
   "source": [
    "## Housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef4b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from graphviz import Source\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3905fbf7",
   "metadata": {},
   "source": [
    "## Preamble - Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c39551bf",
   "metadata": {},
   "source": [
    "To clean our data, we have reused some of the code we had employed earlier on in the course in our first assignment. We first concatenate these files in the below cell block, and run this file through our scrubber which is where we handle our data cleaning. For further details, you can review the scrubber notebook: Data_Scrubber.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7165f338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining the two datasets and saving as a new csv\n",
    "df1 = pd.read_csv('covid19-cdc-22207000.csv', keep_default_na=True, delimiter=',', skipinitialspace=True)\n",
    "df2 = pd.read_csv('covid19-cdc-12468998.csv', keep_default_na=True, delimiter=',', skipinitialspace=True)\n",
    "\n",
    "combined_df = pd.concat([df1, df2])\n",
    "\n",
    "combined_df.to_csv('covid19-cdc-combined-22207000-12468998.csv', index=False)\n",
    "\n",
    "print(len(combined_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06add5d1",
   "metadata": {},
   "source": [
    "The out of our data scrubber is <b>finalised-covid19-cdc-combined-22207000-12468998.csv</b>. This is the file we will use to train/test our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ced43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the post-scrubbed CSV and checking length\n",
    "df = pd.read_csv('finalised-covid19-cdc-combined-22207000-12468998.csv', keep_default_na=True, delimiter=',', skipinitialspace=True)\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98991013",
   "metadata": {},
   "source": [
    "# Task 1: Data Understanding and Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3217a8f4",
   "metadata": {},
   "source": [
    "# 1.1 Train/Test Split\n",
    "\n",
    "With the data cleaning taken care of, we can now conduct a train text split on the data to prepare it for future machine learning tasks. Once the data is split, we can plot the train frame and investigate the relationships between the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4340f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data for task 1\n",
    "df = pd.read_csv('finalised-covid19-cdc-combined-22207000-12468998.csv', keep_default_na=True, delimiter=',', skipinitialspace=True)\n",
    "\n",
    "#train test split on data\n",
    "train, test = train_test_split(df, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3cff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e524462",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a73bb470",
   "metadata": {},
   "source": [
    "# 1.2 Mapping our Training Set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a586122",
   "metadata": {},
   "source": [
    "## Correlation Between Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a21b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(20, 6), ncols=1, nrows=1)\n",
    "fig.set_facecolor('#f5f5f5')\n",
    "\n",
    "plt.scatter(train['case_positive_specimen_interval'], train['case_onset_interval'], c='#84dcc6')\n",
    "corr = train['case_onset_interval'].corr(train['case_positive_specimen_interval'])\n",
    "print('Correlation: ', corr)\n",
    "print('Weak positive correlation between these two variables.')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70f635a1",
   "metadata": {},
   "source": [
    "## Continuous Features Plotted Against Death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c16460",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(20, 6), ncols=1, nrows=1)\n",
    "plt.scatter(train['death_yn'], train['case_positive_specimen_interval'], c='#84dcc6')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f9da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(20, 6), ncols=1, nrows=1)\n",
    "fig.set_facecolor('#f5f5f5')\n",
    "plt.scatter(train['death_yn'], train['case_onset_interval'], c='#84dcc6')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3b894ca",
   "metadata": {},
   "source": [
    "## Health Related Categorical Features Plotted Against Death\n",
    "\n",
    "Here we are looking discern patterns from our dataset by plotting deaths (yes/no) against a select number of categorical features. The features we are going to examine first are those that relate to the health profile of our patients:\n",
    "\n",
    "1. When the patient contracted COVID.\n",
    "2. What US state the patient was in when they were reported of having COVID.\n",
    "3. What age group the patient belongs to.\n",
    "4. Whether the patient knew if they were exposed to COVID.\n",
    "5. Whether or not the patient was hospitalised or not.\n",
    "6. Whether or not the patient was taken to ICU.\n",
    "7. And whether or not the patient exhibited underlying conditions which could exacerbate COVID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1fed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = train['death_yn']\n",
    "select_features = [train['case_month'], train['res_state'], train['age_group'], train['hosp_yn'], train['exposure_yn'], train['icu_yn'], train['underlying_conditions_yn']]\n",
    "\n",
    "colours = ['#84dcc6', '#ffc09f']\n",
    "\n",
    "for feature in select_features:\n",
    "  \n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(20, 8))\n",
    "    train.groupby([feature, target_feature]).size().unstack().plot(kind='bar', stacked=True, ax=axs[0], color=colours)\n",
    "    axs[0].set_xticklabels(axs[0].get_xticklabels(), rotation=90, fontsize=10)\n",
    "    axs[0].set_ylabel('#')\n",
    "\n",
    "    train.groupby([feature, target_feature]).size().unstack().apply(lambda x: x/x.sum(), axis=1).plot(kind='bar', stacked=True, ax=axs[1], color=colours)\n",
    "    axs[1].set_xticklabels(axs[1].get_xticklabels(), rotation=90, fontsize=10)\n",
    "    axs[1].set_ylabel('%')\n",
    "    \n",
    "    fig.set_facecolor('#f5f5f5')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216ea269",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = train['underlying_conditions_yn']\n",
    "select_features = [train['hosp_yn'], train['icu_yn'], train['death_yn']]\n",
    "\n",
    "colours = ['#84dcc6', '#ffc09f']\n",
    "\n",
    "for feature in select_features:\n",
    "  \n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(20, 8))\n",
    "    train.groupby([feature, target_feature]).size().unstack().plot(kind='bar', stacked=True, ax=axs[0], color=colours)\n",
    "    axs[0].set_xticklabels(axs[0].get_xticklabels(), rotation=90, fontsize=10)\n",
    "    axs[0].set_ylabel('#')\n",
    "\n",
    "    train.groupby([feature, target_feature]).size().unstack().apply(lambda x: x/x.sum(), axis=1).plot(kind='bar', stacked=True, ax=axs[1], color=colours)\n",
    "    axs[1].set_xticklabels(axs[1].get_xticklabels(), rotation=90, fontsize=10)\n",
    "    axs[1].set_ylabel('%')\n",
    "    \n",
    "    fig.set_facecolor('#f5f5f5')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d4b9020",
   "metadata": {},
   "source": [
    "## Sociological Related Categorical Features Plotted Against Death\n",
    "\n",
    "Here we are looking discern patterns from our dataset by plotting deaths (yes/no) against a number of sociological features: race, ethnicity, sex.\n",
    "\n",
    "1. The race of the patient.\n",
    "2. The ethnicity of the patient.\n",
    "3. The sex of the patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64697523",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_features = [train['race'], train['ethnicity'], train['sex']]\n",
    "select_features = [train['death_yn']]\n",
    "\n",
    "colours = ['#84dcc6', '#ffc09f']\n",
    "\n",
    "for target_feature in target_features:\n",
    "\n",
    "    for feature in select_features:\n",
    "\n",
    "        fig, axs = plt.subplots(ncols=2, figsize=(20, 8))\n",
    "        train.groupby([target_feature, feature]).size().unstack().plot(kind='bar', stacked=True, ax=axs[0], color=colours)\n",
    "        axs[0].set_xticklabels(axs[0].get_xticklabels(), rotation=90, fontsize=10)\n",
    "        axs[0].set_ylabel('#')\n",
    "\n",
    "        train.groupby([target_feature, feature]).size().unstack().apply(lambda x: x/x.sum(), axis=1).plot(kind='bar', stacked=True, ax=axs[1], color=colours)\n",
    "        axs[1].set_xticklabels(axs[1].get_xticklabels(), rotation=90, fontsize=10)\n",
    "        axs[1].set_ylabel('%')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d853dfd",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "\n",
    "1. As the COVID19 pandemic progressed, we can generally observe both a reduction in the number of deaths per month and the proporption of deaths with regards the whole, month-on-month. The outlier here is January 2022. From the CDC dataset provided, we cannot discern whether or not the patient was vaccinated, however we can use the months here as a proxy for the Vaccine rollout program in the US, which began mid-December 2020 and had more or less concluded by the end of the following year. The death ratio falling against total cases broadly follows the vaccination rollout scheme, despite this information not formally recorded in our dataset. Anecdotally, as COVID19 pandemic persisted more and more COVID mutations occured, often leading the new dominant COVID virus to be more transmissible at the expense of its lethality.\n",
    "\n",
    "2. The data also suggests that age is a predominant and robust factor when gauging the impact contracting COVID will have on your health. Our data shows that those who belong to the oldest age bracket (65+) and test positive for COVID are far more likely to die than those who are younger (compare 65+ against 0-17 years old). Our data also shows that those who are older are more exposed to the risk of COVID, in that the oldest age bucket leads in terms of hospitalisations, ICU admissions, and underlying conditions in comparison to the other age brackets. This aligns with what is now known about COVID-19, that older adults and those with underlying health conditions are considered to be at a higher risk of severe illness and death from COVID-19. \n",
    "\n",
    "3. The data also demonstrates that having underlying medical conditions increases the risk of hospitalisation, ICU admission, and death from COVID-19. And while we do not get access to the comorbidities affecting the patients in our dataset, those who reported as having underlying conditions such as diabtes, hypertension, obesity (etc.), confer a much higher risk of death from COVID-19 than others.\n",
    "\n",
    "4. The data also shows that in the event of being admitted to hospital or the ICU, the success of making a full recovery is limited, and drasticly decreases the older you are. The data also proves the opposite whereby the younger you are the more resilient you are to COVID, leading to far fewer hospitalisation, admissions to ICU, and deaths. \n",
    "\n",
    "5. The data also shows that minorities, categorised in our race and ethnicity features (Black, Hispanic, Asian), are hospitalised, admitted to ICU, and die more regularly than their white counterparts pro rata. Without more granular information on our patient it is hard to say exactly what factors are driving these disparities: access to healthcare; wealth; etc.,\n",
    "\n",
    "6. The data also demonstrates that there is a weak correlation between case_onset_intervals and case_positive_specimen_intervals, with the correlation coefficient is close to zero. This indicates that the timing of when a person experiences symptoms (case onset interval) and when they test positive for COVID-19 (case positive specimen interval) are not strongly associated with each other.\n",
    "\n",
    "7. Our data also shows that there is a weak negative correlation between our continuous features, and when we map these continuous features against our target feature, there is no real discernible pattern that differents yes from no.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50b7bb47",
   "metadata": {},
   "source": [
    "## Selected Features\n",
    "\n",
    "Based on the above observations, the subset of features we are going to use to model the health profile of our patients are: \n",
    "\n",
    "i. <b>Age Group</b>, \n",
    "ii. <b>Case Month</b>, \n",
    "iii. <b>Hospitalisation Status</b>, \n",
    "iv. <b>ICU Status</b>, \n",
    "v. <b>Underlying Conditions</b>. \n",
    "\n",
    "Let's transform these features into new features with discrete values so we can use them in our models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a1c0547",
   "metadata": {},
   "source": [
    "#### Case Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fec46d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_map = {'2020-01': 0, \n",
    "             '2020-02': 1, \n",
    "             '2020-03': 2, \n",
    "             '2020-04': 3,\n",
    "             '2020-05': 4,\n",
    "             '2020-06': 5,\n",
    "             '2020-07': 6,\n",
    "             '2020-08': 7,\n",
    "             '2020-09': 8,\n",
    "             '2020-10': 9,\n",
    "             '2020-11': 10,\n",
    "             '2020-12': 11,\n",
    "             '2021-01': 12,\n",
    "             '2021-02': 13,\n",
    "             '2021-03': 14,\n",
    "             '2021-04': 15,\n",
    "             '2021-05': 16,\n",
    "             '2021-06': 17,\n",
    "             '2021-07': 18,\n",
    "             '2021-08': 19,\n",
    "             '2021-09': 20,\n",
    "             '2021-10': 21,\n",
    "             '2021-11': 22,\n",
    "             '2021-12': 23,\n",
    "             '2022-01': 24,\n",
    "             '2022-02': 25,\n",
    "             '2022-03': 26,\n",
    "             '2022-04': 27,\n",
    "             '2022-05': 28,\n",
    "             '2022-06': 29,\n",
    "             '2022-07': 30,\n",
    "             '2022-08': 31,\n",
    "             '2022-09': 32,\n",
    "             '2022-10': 33,\n",
    "             '2022-11': 34\n",
    "             }\n",
    "\n",
    "train['case_month'] = train['case_month'].map(month_map)\n",
    "test['case_month'] = test['case_month'].map(month_map)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa276b00",
   "metadata": {},
   "source": [
    "#### Age Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333451c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_map = {'0 - 17 years': 0, \n",
    "           '18 to 49 years': 1, \n",
    "           '50 to 64 years': 2, \n",
    "           '65+ years': 3\n",
    "           }\n",
    "\n",
    "train['age_group'] = train['age_group'].map(age_map)\n",
    "test['age_group'] = test['age_group'].map(age_map)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a092cab8",
   "metadata": {},
   "source": [
    "#### Hospitalisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5292c176",
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_map = { 'Unknown': 0, \n",
    "            'Yes': 1, \n",
    "            }\n",
    "\n",
    "train['hosp_yn'] = train['hosp_yn'].map(hosp_map)\n",
    "test['hosp_yn'] = test['hosp_yn'].map(hosp_map)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c277322",
   "metadata": {},
   "source": [
    "#### ICU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c604d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_map = {'Unknown': 0, \n",
    "            'Yes': 1, \n",
    "            }\n",
    "\n",
    "train['icu_yn'] = train['icu_yn'].map(icu_map)\n",
    "test['icu_yn'] = test['icu_yn'].map(icu_map)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3992f752",
   "metadata": {},
   "source": [
    "### Exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e103d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "exposure_map = {'Unknown': 0, \n",
    "            'Yes': 1, \n",
    "            }\n",
    "\n",
    "train['exposure_yn'] = train['exposure_yn'].map(icu_map)\n",
    "test['exposure_yn'] = test['exposure_yn'].map(icu_map)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "339e296d",
   "metadata": {},
   "source": [
    "#### Underlying Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7302fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "underlying_map = {'Unknown': 0, \n",
    "                  'Yes': 1, \n",
    "                }\n",
    "\n",
    "train['underlying_conditions_yn'] = train['underlying_conditions_yn'].map(underlying_map)\n",
    "test['underlying_conditions_yn'] = test['underlying_conditions_yn'].map(underlying_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb9a8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['definite_underlying'] = (train['underlying_conditions_yn'] == 1).astype(int)\n",
    "# train['uncertain_underlying'] = (train['underlying_conditions_yn'] == 0).astype(int)\n",
    "\n",
    "# test['definite_underlying'] = (test['underlying_conditions_yn'] == 1).astype(int)\n",
    "# test['uncertain_underlying'] = (test['underlying_conditions_yn'] == 0).astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6617f1e6",
   "metadata": {},
   "source": [
    "#### Death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f0451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "death_map = {'No': 0,\n",
    "             'Yes': 1,\n",
    "             }\n",
    "\n",
    "train['death_yn'] = train['death_yn'].map(death_map)\n",
    "test['death_yn'] = test['death_yn'].map(death_map)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb4498cd",
   "metadata": {},
   "source": [
    "# Task 2: Predictive Modeling - Linear Regression\n",
    "\n",
    "The aim of this task is to perform some basic predictive analysis on the data prepared in task one. To this end, a linear regression model will be created using the training data yielded by the train-test split from the previous task. The features to be analyzed in this section are:\n",
    "\n",
    "i. <b>age</b>, \n",
    "ii. <b>case month</b>, \n",
    "iii. <b>hospilisation status</b>, \n",
    "iv. <b>ICU status</b>, \n",
    "v. <b>underlying conditions</b>. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd2b8201",
   "metadata": {},
   "source": [
    "## 2.1 Training our Linear Regression Model\n",
    "\n",
    "Here we train our linear regression model based on the features we selected above. We train this model against our target feature which is death."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2f9b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_x_train = train[['case_month', 'age_group', 'hosp_yn', 'icu_yn', 'underlying_conditions_yn']]\n",
    "linear_regression_y_train = train['death_yn']\n",
    "\n",
    "linear_regression_x_test = test[['case_month', 'age_group', 'hosp_yn', 'icu_yn', 'underlying_conditions_yn']]\n",
    "linear_regression_y_test = test['death_yn']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3152e9a",
   "metadata": {},
   "source": [
    "Sanity checking feature selection for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3dd7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linear_regression_x_train.shape)\n",
    "print(linear_regression_x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a313d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(linear_regression_x_train, linear_regression_y_train)\n",
    "linear_regression_y_pred = lr.predict(linear_regression_x_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5279e90e",
   "metadata": {},
   "source": [
    "## 2.2 Evaluating the Coefficients of our Linear Regression Model\n",
    "\n",
    "Here we print the coefficients of each feature we used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c3cf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_coeff = pd.DataFrame({'Feature': linear_regression_x_train.columns, 'Coeff': lr.coef_})\n",
    "lr_coeff.reset_index(drop=True, inplace=True)\n",
    "lr_coeff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d857a5b",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "1. <b>Case Month</b>: The coefficient for Case Month is -0.006671, which indicates that there is a small negative correlation between the month of the reported case and the target variable. That is to say that as the month of the reported case increases, the value of the target variable is expected to decrease slightly. This makes sense as we can understand time here to be a proxy (of sorts) for vaccination and the decreasing lethality of COVID19 as it mutated over the course of 34 months.\n",
    "\n",
    "2. <b>Age Group</b>: The coefficient for this feature is 0.198640, which indicates a positive correlation between the age group and the target variable. In other words, as the age group of the patient increases, the value of the target variable is expected to increase. This follows the trends we observe from our dataset where the older the patient the more likely they are of dying from COVID19.\n",
    "\n",
    "3. <b>Hospitlisation Status</b>: This feature represents whether the patient was hospitalised for COVID19 or not. As we might expect, the coefficient for this feature is relatively high at 0.413565, which demonstrates a strong positive correlation between hospitalisation and the target variable, death. \n",
    "\n",
    "4. <b> ICU Status </b>: Represents whether the patient was admitted to the ICU or not. The coefficient for this feature is 0.147415, which indicates a relatively strong positive correlation between ICU admission and our target variable, but not as strong as hospitalisation. However, we can understand this feature as a subset of hospilisation, as 99.8% of our patients who were admitted to ICU were first hospitalised, which in part can help us understand why this value is weighted less than hospitalisation.\n",
    "\n",
    "5. <b> Underlying Health Conditions </b>: The coefficient for this feature is confusing at the model infers a negative correlation to having underlying health conditions. This finding is counterintuitive, as individuals with underlying health conditions are often thought to be at higher risk for adverse outcomes such as hospitalisation or death from COVID-19. However, it is important to remember that the coefficient represents the association between the predictor variable and the target outcome after accounting for the effects of other predictor variables in the model. In this case, it is possible that other predictor variables, such as age or hospitalisation status, may be more strongly associated with the target outcome than underlying health condition, and that the effect of underlying health condition on the target outcome is actually negative when these other factors are taken into account. Therefore, the negative coefficient for \"definite_underlying\" suggests that the presence of a definite underlying health condition alone may not be a strong predictor of the target outcome in this model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c8ee36c",
   "metadata": {},
   "source": [
    "## 2.3.1 Target Value and Class Prediction\n",
    "\n",
    "Here we print the predicted target feature value for the first 10 training examples, and the predicted class for the first 10 training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8abebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict target feature values for first 10 training examples\n",
    "linear_regression_y_train_pred_10 = lr.predict(linear_regression_x_train[:10])\n",
    "y_train_pred_df = pd.DataFrame({'Predicted target feature values': linear_regression_y_train_pred_10})\n",
    "\n",
    "# threshold predicted target feature values to get predicted classes\n",
    "linear_regression_y_train_pred_class_10 = (lr.predict(linear_regression_x_train[:10]) > 0.5).astype(int)\n",
    "y_train_pred_df['Predicted class'] = linear_regression_y_train_pred_class_10\n",
    "\n",
    "print(y_train_pred_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfa20cc4",
   "metadata": {},
   "source": [
    "## 2.3.2 Classification Evaluation Measures on Training Set\n",
    "\n",
    "Below we print the classification evaluation measures computed on the full training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81575fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on full training set\n",
    "linear_regression_y_train_pred = lr.predict(linear_regression_x_train)\n",
    "\n",
    "linear_regression_y_train_pred_class = (linear_regression_y_train_pred > 0.5).astype(int)\n",
    "\n",
    "linear_regression_accuracy_train = accuracy_score(linear_regression_y_train, linear_regression_y_train_pred_class)\n",
    "linear_regression_precision_train = precision_score(linear_regression_y_train, linear_regression_y_train_pred_class)\n",
    "linear_regression_recall_train = recall_score(linear_regression_y_train, linear_regression_y_train_pred_class)\n",
    "linear_regression_f1_train = f1_score(linear_regression_y_train, linear_regression_y_train_pred_class)\n",
    "\n",
    "linear_regression_confusion_train = confusion_matrix(linear_regression_y_train, linear_regression_y_train_pred_class)\n",
    "\n",
    "linear_regression_mse_train = mean_squared_error(linear_regression_y_train, linear_regression_y_train_pred_class)\n",
    "linear_regression_mae_train = mean_absolute_error(linear_regression_y_train, linear_regression_y_train_pred_class)\n",
    "linear_regression_rmse_train = np.sqrt(linear_regression_mse_train)\n",
    "linear_regression_r2_train = r2_score(linear_regression_y_train, linear_regression_y_train_pred_class)\n",
    "\n",
    "print(\"\\nEvaluation metrics on the training set:\")\n",
    "print()\n",
    "print(\"Accuracy: {:.2f}\".format(linear_regression_accuracy_train))\n",
    "print(\"Precision: {:.2f}\".format(linear_regression_precision_train))\n",
    "print(\"Recall: {:.2f}\".format(linear_regression_recall_train))\n",
    "print(\"F1 score: {:.2f}\".format(linear_regression_f1_train))\n",
    "print()\n",
    "print(\"Confusion matrix:\\n\", linear_regression_confusion_train)\n",
    "print()\n",
    "print(\"Mean Squared Error: {:.2f}\".format(linear_regression_mse_train))\n",
    "print(\"Mean Absolute Error: {:.2f}\".format(linear_regression_mae_train))\n",
    "print(\"R^2: {:.2f}\".format(linear_regression_r2_train))\n",
    "print(\"Root Mean Squared Error: {:.2f}\".format(linear_regression_rmse_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f73118b0",
   "metadata": {},
   "source": [
    "Adding all of our performance metrics to an iterable so we can loop over them for graph generation later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf8886",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_linear_regression_training_stats = [linear_regression_accuracy_train, linear_regression_precision_train, linear_regression_recall_train, linear_regression_mse_train, linear_regression_mae_train, linear_regression_rmse_train, linear_regression_r2_train]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0d032b9",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "\n",
    "Based on the provided evaluation metrics, the model seems to quite well on our training set. \n",
    "\n",
    "1. The <b>Accuracy</b> of 0.88 indicates that the model predicted correctly 88% of the time. \n",
    "2. <b>Precision</b> is a measure of how many of the instances that the model predicted as positive were actually positive. A precision score of 0.81 means that out of all the instances that the model predicted as positive, 87% of the time the instance was actually positive.\n",
    "3. For <b>Recall</b> we measured quite poorly, all things considered. A recall score of 0.65 means that the model correctly identified 65% of all the actual positive instances.\n",
    "4. The <b>F1 Score</b> of 0.75 is a harmonic mean of precision and recall, which is a useful metric when there is an imbalance in the number of instances of each class. In this case, the F1 score indicates a good balance between precision and recall.\n",
    "5. An <b>MSE</b> of 0.12 represents the average squared difference between the predicted and actual outcome values. Our low MSE score here is indicative of decent performance.\n",
    "6. The R^2 value of 0.40 indicates that 40% of the variation in the outcome variable can be explained by the predictor variables included in the model.\n",
    "7. Our <b>Confusion Matrix</b> is a visualisation of the distribution of correct and incorrect predictions across the two classes in table format. In this case, the model made 17585 true negative predictions, 649 false negative predictions, 2251 false positive predictions, and 4258 true positive predictions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<b>Finish commmentary here on R2 etc.....</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025869ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear Regression Classification Report:\\n\", classification_report(linear_regression_y_train, linear_regression_y_train_pred_class))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6af44122",
   "metadata": {},
   "source": [
    "## 2.4.1 Classification Evaluation Comparison Between Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9a39e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate model on full test set\n",
    "linear_regression_y_test_pred = lr.predict(linear_regression_x_test)\n",
    "\n",
    "linear_regression_y_test_pred_class = (linear_regression_y_test_pred > 0.5).astype(int)\n",
    "\n",
    "linear_regression_accuracy_test = accuracy_score(linear_regression_y_test, linear_regression_y_test_pred_class)\n",
    "linear_regression_precision_test = precision_score(linear_regression_y_test, linear_regression_y_test_pred_class)\n",
    "linear_regression_recall_test = recall_score(linear_regression_y_test, linear_regression_y_test_pred_class)\n",
    "linear_regression_f1_test = f1_score(linear_regression_y_test, linear_regression_y_test_pred_class)\n",
    "\n",
    "linear_regression_confusion_test = confusion_matrix(linear_regression_y_test, linear_regression_y_test_pred_class)\n",
    "\n",
    "linear_regression_mae_test = mean_absolute_error(linear_regression_y_test, linear_regression_y_test_pred_class)\n",
    "linear_regression_mse_test = mean_squared_error(linear_regression_y_test, linear_regression_y_test_pred_class)\n",
    "linear_regression_rmse_test = np.sqrt(linear_regression_mse_test)\n",
    "linear_regression_r2_test = r2_score(linear_regression_y_test, linear_regression_y_test_pred_class)\n",
    "\n",
    "print(\"\\nEvaluation metrics on the test set:\")\n",
    "print(\"Accuracy: {:.2f}\".format(linear_regression_accuracy_test))\n",
    "print(\"Precision: {:.2f}\".format(linear_regression_precision_test))\n",
    "print(\"Recall: {:.2f}\".format(linear_regression_recall_test))\n",
    "print(\"F1 score: {:.2f}\".format(linear_regression_f1_test))\n",
    "print()\n",
    "print(\"Confusion matrix:\\n\", linear_regression_confusion_test)\n",
    "print()\n",
    "print(\"Mean Squared Error: {:.2f}\".format(linear_regression_mse_test))\n",
    "print(\"Mean Absolute Error: {:.2f}\".format(linear_regression_mae_train))\n",
    "print(\"R^2: {:.2f}\".format(linear_regression_r2_test))\n",
    "print(\"Root Mean Squared Error: {:.2f}\".format(linear_regression_rmse_test))\n",
    "\n",
    "list_of_linear_regression_testing_stats = [linear_regression_accuracy_test, linear_regression_precision_test, linear_regression_recall_test, linear_regression_mse_test, linear_regression_mae_test, linear_regression_rmse_test, linear_regression_r2_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82caa01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report:\\n\", classification_report(linear_regression_y_test, linear_regression_y_test_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dbe6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['Accuracy', 'Precision', 'Recall', 'MSE', 'MAE', 'RMSE', 'R2']\n",
    "\n",
    "for i in range(len(metrics)):\n",
    "    print(metrics[i])\n",
    "    print('='*30)\n",
    "    print('Trained Data: ', list_of_linear_regression_training_stats[i])\n",
    "    print('Tested Data: ', list_of_linear_regression_testing_stats[i])\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ab10d34",
   "metadata": {},
   "source": [
    "As might be expected, the model performance on the training data and test data was incredibly similar. Both confusion matrices performed well on predicting negatives, but performed slightly worse when detecting positives. Metrics like accuracy, presicion, F1, and MSE proved identical, with only minor differences in Recall and R^2 to the tune of .01. \n",
    "\n",
    "Overall, performance on both the testing and training data seems to indicate a model with generally high accuracy, and precision, reacall, and F1 scores in excess of 90%. This kind of performance bodes well for a linear regression model's potential as a best fit for this particular problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "164270b3",
   "metadata": {},
   "source": [
    "## 2.4.2 K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0089bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing 5 fold K-fold cross validation on linear regression model using full dataset\n",
    "\n",
    "#concatenating train and test datasets for independent and dependent variables\n",
    "X = pd.concat([train[['case_month', 'age_group', 'hosp_yn', 'icu_yn', 'underlying_conditions_yn']], \n",
    "                                 test[['case_month', 'age_group', 'hosp_yn', 'icu_yn', 'underlying_conditions_yn']]])\n",
    "y = pd.concat([train['death_yn'], test['death_yn']]) \n",
    "\n",
    "#folds for cross validation\n",
    "folds = 5\n",
    "\n",
    "#creating K-Fold object\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "\n",
    "#K-fold cross validation scores\n",
    "mse_scores = -cross_val_score(lr, X, y, cv=folds, scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "#printing results\n",
    "print(f\"MSE scores: {mse_scores}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9559f4e8",
   "metadata": {},
   "source": [
    "The MSE scores produced by the K-Fold cross valdation process performed on the linear regression model were generally very low, with none exceeding 0.083. This indicates that the linear regression implementation is relatively sucessful and predicting the dependent outcome based on the provided independent features, even when performed on subsets of the full dataset. This is consistent with the relatively high accuracy yielded by the model when performing analysis of the testing and training data. Taken together, the model performance information and the K-fold cross validation information give us a good indication that this model would likely perform well on new data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51eee504",
   "metadata": {},
   "source": [
    "# Task 3: Predictive Modeling - Logistic Regression\n",
    "\n",
    "The aim of this task is to perform some basic predictive analysis on the data prepared in task one. To this end, a logistic regression model will be created using the training data yielded by the train-test split from the previous task. The features to be analyzed in this section are:\n",
    "\n",
    "i. <b>age</b>, \n",
    "ii. <b>case month</b>, \n",
    "iii. <b>hospilisation status</b>, \n",
    "iv. <b>ICU status</b>, \n",
    "v. <b>underlying conditions</b>. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aeeec765",
   "metadata": {},
   "source": [
    "## 3.1 Training our Logistic Regression Model\n",
    "\n",
    "Here we train our logistic regression model based on the features we selected above. We train this model against our target feature which is death."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a5acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_x_train = train[['case_month', 'age_group', 'hosp_yn', 'icu_yn', 'underlying_conditions_yn']]\n",
    "logistic_regression_y_train = train['death_yn']\n",
    "\n",
    "logistic_regression_x_test = test[['case_month', 'age_group', 'hosp_yn', 'icu_yn', 'underlying_conditions_yn']]\n",
    "logistic_regression_y_test = test['death_yn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d49b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "logr = LogisticRegression()\n",
    "logr.fit(logistic_regression_x_train, logistic_regression_y_train)\n",
    "logistic_regression_y_pred = logr.predict(logistic_regression_x_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15f3680d",
   "metadata": {},
   "source": [
    "Sanity checking feature selection for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eb0964",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logistic_regression_x_train.shape)\n",
    "print(logistic_regression_x_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f16e17a",
   "metadata": {},
   "source": [
    "## 3.2 Evaluating the Coefficients of our Logistic Regression Model\n",
    "\n",
    "Here we print the coefficients of each feature we used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee439cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_coeff = pd.DataFrame({'Feature': logistic_regression_x_train.columns, 'Coeff': logr.coef_.ravel()})\n",
    "logr_coeff.reset_index(drop=True, inplace=True)\n",
    "logr_coeff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "639d27e1",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "\n",
    "1. Similar to our Linear Regression model, the negative coefficient of -0.084 indicates that there is an inverse relationship between <b>Case Month</b> and the positive class probability. That is to say that when the case month increases, the log-odds of the positive class decreases. This again complements our initial observations that case month is a fair proxy for baseline vaccination percentages across the american population and the decreasing lethality of the COVID19 pathogen.\n",
    "2. Again similar to our Linear Regression Model, the Logistic Regression Model weights <b>Age Group</b> quite heavilty. The positive coefficient 2.588 indicates that as the age of our patient increases, the log-odds of the positive class also increase. This means there is a direct relationship between the age of our patient andthe positive class probability. Suffice it to say that older patients are more likely to die than younger patients of COVID.\n",
    "3. The Logistic Regression Model also weights <b>Hospilisation Status</b> and <b>ICU Status</b> quite highly, with coefficients of 2.507 and 2.322 respectively. This infers that when a patient is hospitalised, or subsequently taken to the ICU after their hospitalisation, are more likely to die from COVID than those who have not rushed to hospital or been admitted to an ICU.\n",
    "4. Again the cresults for <b>Underlying Conditions</b> seem to be rather counter-intuitive, as our Logistic Regression Model demonstrates that patients with underlying conditions are less likely to die than those without underlying conditions. This is reflected in the weak negative coefficient of -0.647. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6282ae30",
   "metadata": {},
   "source": [
    "## 3.3.1 Target Value and Class Prediction\n",
    "\n",
    "Here we print the predicted target feature value for the first 10 training examples, and the predicted class for the first 10 training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e07e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_y_train_pred_10 = logr.predict_proba(logistic_regression_x_train)[:, 1][:10]\n",
    "logistic_regression_y_train_pred_class_10 = (logr.predict(logistic_regression_x_train[:10]) >= 0.5).astype(int)\n",
    "\n",
    "logistic_regression_output_df = pd.DataFrame({\n",
    "    \"Predicted Values\": logistic_regression_y_train_pred_10,\n",
    "    \"Predicted Class\": logistic_regression_y_train_pred_class_10\n",
    "})\n",
    "\n",
    "print(logistic_regression_output_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "060374d5",
   "metadata": {},
   "source": [
    "## 3.3.2 Classification Evaluation Measures on Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c4c8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict target feature values for full training set\n",
    "logistic_regression_y_train_pred = logr.predict_proba(logistic_regression_x_train)[:, 1]\n",
    "\n",
    "# threshold predicted target feature values at 0.5 to get predicted class for full training set\n",
    "logistic_regression_y_train_pred_class = (logistic_regression_y_train_pred >= 0.5).astype(int)\n",
    "\n",
    "logistic_regression_accuracy_train = accuracy_score(logistic_regression_y_train, logistic_regression_y_train_pred_class)\n",
    "logistic_regression_confusion_train = confusion_matrix(logistic_regression_y_train, logistic_regression_y_train_pred_class)\n",
    "logistic_regression_recall_train = recall_score(logistic_regression_y_train, logistic_regression_y_train_pred_class)\n",
    "logistic_regression_f1_train = f1_score(logistic_regression_y_train, logistic_regression_y_train_pred_class)\n",
    "\n",
    "logistic_regression_precision_train = precision_score(logistic_regression_y_train, logistic_regression_y_train_pred_class)\n",
    "\n",
    "logistic_regression_mae_train = mean_absolute_error(logistic_regression_y_train, logistic_regression_y_train_pred)\n",
    "logistic_regression_mse_train = mean_squared_error(logistic_regression_y_train, logistic_regression_y_train_pred)\n",
    "logistic_regression_rmse_train = np.sqrt(logistic_regression_mse_train)\n",
    "logistic_regression_r2_train = r2_score(logistic_regression_y_train, logistic_regression_y_train_pred)\n",
    "\n",
    "print(\"\\nEvaluation metrics on the training set:\")\n",
    "print()\n",
    "print(\"Accuracy: {:.2f}\".format(logistic_regression_accuracy_train))\n",
    "print(\"Precision: {:.2f}\".format(logistic_regression_precision_train))\n",
    "print(\"Recall: {:.2f}\".format(logistic_regression_recall_train))\n",
    "print(\"F1 score: {:.2f}\".format(logistic_regression_f1_train))\n",
    "print()\n",
    "print(\"Confusion matrix:\\n\", logistic_regression_confusion_train)\n",
    "print()\n",
    "print(\"Mean Squared Error: {:.2f}\".format(logistic_regression_mse_train))\n",
    "print(\"Mean Absolute Error: {:.2f}\".format(logistic_regression_mae_train))\n",
    "print(\"R2 score: {:.2f}\".format(logistic_regression_r2_train))\n",
    "print(\"Root Mean Squared Error: {:.2f}\".format(logistic_regression_rmse_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44ef3b2f",
   "metadata": {},
   "source": [
    "Adding all of our performance metrics to an iterable so we can loop over them for graph generation later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000496cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_logistic_regression_training_stats = [logistic_regression_accuracy_train, logistic_regression_precision_train, logistic_regression_recall_train, logistic_regression_mse_train, logistic_regression_mae_train, logistic_regression_rmse_train, logistic_regression_r2_train]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7cd9d83d",
   "metadata": {},
   "source": [
    "## 3.4.1: Classification Evaluation Comparison Between Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204d7606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on full test set\n",
    "logistic_regression_y_test_pred = logr.predict_proba(logistic_regression_x_test)[:, 1]\n",
    "\n",
    "# threshold predicted target feature values at 0.5 to get predicted class for full test set\n",
    "logistic_regression_y_test_pred_class = (logistic_regression_y_test_pred >= 0.5).astype(int)\n",
    "\n",
    "logistic_regression_accuracy_test = accuracy_score(logistic_regression_y_test, logistic_regression_y_test_pred_class)\n",
    "logistic_regression_precision_test = precision_score(logistic_regression_y_test, logistic_regression_y_test_pred_class)\n",
    "logistic_regression_recall_test = recall_score(logistic_regression_y_test, logistic_regression_y_test_pred_class)\n",
    "logistic_regression_f1_test = f1_score(logistic_regression_y_test, logistic_regression_y_test_pred_class)\n",
    "\n",
    "logistic_regression_confusion_test = confusion_matrix(logistic_regression_y_test, logistic_regression_y_test_pred_class)\n",
    "\n",
    "logistic_regression_mae_test = mean_absolute_error(logistic_regression_y_test, logistic_regression_y_test_pred)\n",
    "logistic_regression_mse_test = mean_squared_error(logistic_regression_y_test, logistic_regression_y_test_pred)\n",
    "logistic_regression_rmse_test = np.sqrt(logistic_regression_mse_test)\n",
    "logistic_regression_r2_test = r2_score(logistic_regression_y_test, logistic_regression_y_test_pred)\n",
    "\n",
    "print(\"\\nEvaluation metrics on the test set:\")\n",
    "print()\n",
    "print(\"Accuracy: {:.2f}\".format(logistic_regression_accuracy_test))\n",
    "print(\"Precision: {:.2f}\".format(logistic_regression_precision_test))\n",
    "print(\"Recall: {:.2f}\".format(logistic_regression_recall_test))\n",
    "print(\"F1 score: {:.2f}\".format(logistic_regression_f1_test))\n",
    "print()\n",
    "print(\"Confusion matrix:\\n\", logistic_regression_confusion_test)\n",
    "print()\n",
    "print(\"Mean Absolute Error: {:.2f}\".format(logistic_regression_mae_test))\n",
    "print(\"Mean Squared Error: {:.2f}\".format(logistic_regression_mse_test))\n",
    "print(\"Root Mean Squared Error: {:.2f}\".format(logistic_regression_rmse_test))\n",
    "print(\"R2 score: {:.2f}\".format(logistic_regression_r2_test))\n",
    "\n",
    "list_of_logistic_regression_testing_stats = [logistic_regression_accuracy_test, logistic_regression_precision_test, logistic_regression_recall_test, logistic_regression_mse_test, logistic_regression_mae_test, logistic_regression_rmse_test, logistic_regression_r2_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ec1a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report:\\n\", classification_report(logistic_regression_y_test, logistic_regression_y_test_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455dab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['Accuracy', 'Precision', 'Recall', 'MSE', 'MAE', 'RMSE', 'R2']\n",
    "\n",
    "for i in range(len(metrics)):\n",
    "    print(metrics[i])\n",
    "    print('='*30)\n",
    "    print('Trained Data: ', list_of_logistic_regression_training_stats[i])\n",
    "    print('Tested Data: ', list_of_logistic_regression_testing_stats[i])\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d0b2a58",
   "metadata": {},
   "source": [
    "Again, the performance of the logistic regression model between training and test data is quite similar. Accuracy and F1 score remain consistent, while precision and recall are again off by only .01. Performance on the confusion matrices are, again, very similar too. Overll, the metrics fromt he training and testing data indicate a well-performing model, at least on mpar with if not slightly superior to the linear regression model. This outcome might be expected with a logistic regression model, due to the binary nature of the dependent feature as well as several of the independent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab97675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing 5 fold K-fold cross validation on logistic regression model using full dataset\n",
    "\n",
    "#concatenating train and test datasets for independent and dependent variables\n",
    "X = pd.concat([train[['case_month', 'age_group', 'hosp_yn', 'icu_yn', 'underlying_conditions_yn']], \n",
    "                                 test[['case_month', 'age_group', 'hosp_yn', 'icu_yn', 'underlying_conditions_yn']]])\n",
    "y = pd.concat([train['death_yn'], test['death_yn']]) \n",
    "\n",
    "#folds for cross validation\n",
    "folds = 5\n",
    "\n",
    "#creating K-Fold object\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "\n",
    "#K-fold cross validation scores\n",
    "mse_scores = -cross_val_score(logr, X, y, cv=folds, scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "#printing results\n",
    "print(f\"MSE scores: {mse_scores}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a40c501a",
   "metadata": {},
   "source": [
    "The MSE scores produced by the K-Fold cross validation process on the logistic regression model were again very low, with none exceeding 0.11. his indicates that the linear regression implementation is relatively sucessful and predicting the dependent outcome based on the provided independent features, even when performed on subsets of the full dataset. This is consistent with the relatively high accuracy yielded by the model when performing analysis of the testing and training data. Taken together, the model performance information and the K-fold cross validation information give us a good indication that this model would likely perform well on new data. \n",
    "\n",
    "One thing to note, however, is that the MSE scores produced were slightly lower than those of the linear regression model's cross validation data. This could indicate there the logistic regression model has some comparatively negative model performance, such as overfitting to certain parts of the data, or not performing as well on new data. Regardless, the accuracy remains high and the MSE scores remain low, indicating good model performance overall."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8441f801",
   "metadata": {},
   "source": [
    "# Task 4: Predictive Modeling - Random Forest Classifier\n",
    "\n",
    "The aim of this task is to perform some basic predictive analysis on the data prepared in task one. To achieve thi we are going to create a random forest classifer model. Again the features we are going to use to train this model are:\n",
    "\n",
    "i. <b>age</b>, \n",
    "ii. <b>case month</b>, \n",
    "iii. <b>hospilisation status</b>, \n",
    "iv. <b>ICU status</b>, \n",
    "v. <b>underlying conditions</b>. \n",
    "\n",
    " The model will then be evaluated based on its performance using the training data, the test data, and other linear regression models trained on the full dataset. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67bd3b80",
   "metadata": {},
   "source": [
    "## 4.1\n",
    "\n",
    "Here we train our random forest classifier model based on the features we selected above. We train this model against our target feature which is death."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc60450",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_x_train = train[['case_month', 'age_group', 'hosp_yn', 'icu_yn', 'underlying_conditions_yn']]\n",
    "random_forest_y_train = train['death_yn']\n",
    "\n",
    "random_forest_x_test = test[['case_month', 'age_group', 'hosp_yn', 'icu_yn', 'underlying_conditions_yn']]\n",
    "random_forest_y_test = test['death_yn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d5c8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rfc.fit(random_forest_x_train, random_forest_y_train)\n",
    "random_forest_y_pred = rfc.predict(random_forest_x_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b60bc6f",
   "metadata": {},
   "source": [
    "## 4.2.1 How does Random Forest Work?\n",
    "\n",
    "Random forest is an ensemble learning algorithm that creates multiple decision trees and then aggregates their results to make a final prediction. It works by creating a set of decision trees, where each tree is trained on a random subset of the data and a random subset of the features. During training, the algorithm chooses the best feature to split the data on, and then continues splitting until the data is fully separated, or a stopping criterion is met.\n",
    "\n",
    "Once the decision trees are built, the algorithm aggregates their results to make a final prediction. As we are using a random forest classifier model, this is done by taking the mode of the predicted classes across all the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b79f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "decision_tree.fit(random_forest_x_train, random_forest_y_train)\n",
    "\n",
    "dot_data = export_graphviz(decision_tree, \n",
    "                           out_file=None, \n",
    "                           feature_names=random_forest_x_train.columns, \n",
    "                           class_names=['0', '1'],\n",
    "                           filled=True, \n",
    "                           rounded=True, \n",
    "                           special_characters=True)\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "\n",
    "graph.set_size('\"20,8!\"')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e3c06f6",
   "metadata": {},
   "source": [
    "## 4.2.2 Evaluating the Feature Importance of our Random Forest Classifier Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3931368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({'Feature': random_forest_x_train.columns, 'Significance':rfc.feature_importances_})\n",
    "feature_importance.sort_values('Significance', ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52e02356",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "1. <b>Age Group here</b> leads the way in terms of feature importance, such that it is the dominant feature in terms of its contribution to the prediction model. It has an importance value of 0.537, which is significantly higher than the second most important feature, <b>hospitalisation Status</b>, with an importance value of 0.302. This matches our previous analysis where we observed that those who belonged to the oldest age bracket were the most exposed group to COVID, and resulted in the highest amount deaths.\n",
    "2. <b>Hospitalisation Status</b> is our second most important feature and from previous analysis we have observed the high correlation between hospitlisation and the target feature. Here our models weights this feature quite heavily, which follows our previous observations and analysis; those who were admitted to hospital for COVID more than likely had a more serious case of COVID19 than those who were not admited to hospital. \n",
    "3. <b>Case Month</b> is weighted more highly here than it has been in other models. This is quite interesting, as earlier we posited that case month could potentially serve as  a proxy for COVID virus lethality (given the continuous evolution and mutation of the dominant COVID pathogen) and general vaccination rates (US vaccine program began in December 2020). While this is somewhat supposition, it is interesting that our model has picked up on this feature as being more imporant than both <b>ICU Status</b> and <b>Underlying Conditions</b>\n",
    "4. Intersting that both <b>ICU Status</b> and <b>Underlying Conditions</b> were weighted quite lowly in this model. Again, this seems counter-intuitive as these are both important factors in determining the severity of COVID-19 cases. However, it's important to note that feature importance in a machine learning model is relative to the other features included in the model. So, while ICU status and underlying conditions may be important factors in determining the severity of COVID-19 cases, they may not be the most important factors in relation to the other features included in the model. It's also possible that the data used to train the model did not have a strong enough relationship between ICU status/underlying conditions and the target variable (severity) for these features to have a high weight in the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ca971ee",
   "metadata": {},
   "source": [
    "## 4.3.1\n",
    "\n",
    "Here we print the predicted target feature value for the first 10 training examples, and the predicted class for the first 10 training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a3aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_y_train_pred_10 = rfc.predict_proba(random_forest_x_train)[:, 1][:10]\n",
    "random_forest_y_train_pred_class_10 = (rfc.predict(random_forest_x_train[:10]) > 0.5).astype(int)\n",
    "\n",
    "random_forest_output_df = pd.DataFrame({\n",
    "    \"Predicted Values\": random_forest_y_train_pred_10,\n",
    "    \"Predicted Class\": random_forest_y_train_pred_class_10\n",
    "})\n",
    "\n",
    "print(random_forest_output_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8aa85c3e",
   "metadata": {},
   "source": [
    "## 4.3.2 Classification Evaluation Measures on Training Set\n",
    "\n",
    "Here we print a few classification evaluation measures computed on the full training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0da5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict target feature values for full training set\n",
    "random_forest_y_train_pred = rfc.predict_proba(random_forest_x_train)[:, 1]\n",
    "\n",
    "# threshold predicted target feature values at 0.5 to get predicted class for full training set\n",
    "random_forest_y_train_pred_class = (random_forest_y_train_pred >= 0.5).astype(int)\n",
    "\n",
    "random_forest_accuracy_train = accuracy_score(random_forest_y_train, random_forest_y_train_pred_class)\n",
    "random_forest_precision_train = precision_score(random_forest_y_train, random_forest_y_train_pred_class)\n",
    "random_forest_recall_train = recall_score(random_forest_y_train, random_forest_y_train_pred_class)\n",
    "random_forest_f1_train = f1_score(random_forest_y_train, random_forest_y_train_pred_class)\n",
    "\n",
    "random_forest_confusion_train = confusion_matrix(random_forest_y_train, random_forest_y_train_pred_class)\n",
    "\n",
    "random_forest_mae_train = mean_absolute_error(random_forest_y_train, random_forest_y_train_pred_class)\n",
    "random_forest_mse_train = mean_squared_error(random_forest_y_train, random_forest_y_train_pred_class)\n",
    "random_forest_rmse_train = np.sqrt(random_forest_mse_train)\n",
    "random_forest_r2_train = r2_score(random_forest_y_train, random_forest_y_train_pred_class)\n",
    "\n",
    "print(\"\\nEvaluation metrics on the training set:\")\n",
    "print(\"Accuracy: {:.2f}\".format(random_forest_accuracy_train))\n",
    "print(\"Precision: {:.2f}\".format(random_forest_precision_train))\n",
    "print(\"Recall: {:.2f}\".format(random_forest_recall_train))\n",
    "print(\"F1 score: {:.2f}\".format(random_forest_f1_train))\n",
    "print()\n",
    "print(\"Confusion matrix:\\n\", random_forest_confusion_train)\n",
    "print()\n",
    "print(\"Mean Squared Error: {:.2f}\".format(random_forest_mse_train))\n",
    "print(\"Mean Absolute Error: {:.2f}\".format(random_forest_mae_train))\n",
    "print(\"R2 score: {:.2f}\".format(random_forest_r2_train))\n",
    "print(\"Root Mean Squared Error: {:.2f}\".format(random_forest_rmse_train))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3959de43",
   "metadata": {},
   "source": [
    "Again, adding our performance metrics to an iterable so we can loop over them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e99f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_random_forest_training_stats = [random_forest_accuracy_train, random_forest_precision_train, random_forest_recall_train, random_forest_mse_train, random_forest_mae_train, random_forest_rmse_train, random_forest_r2_train]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bd069f0",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "Based on the provided evaluation metrics, the model seems to perform well on the training set. \n",
    "\n",
    "1. The <b>accuracy</b> of 0.91 indicates that the model predicted correctly 91% of the time. \n",
    "2. <b>Precision</b> is a measure of how many of the instances that the model predicted as positive were actually positive. In other words, it measures how many true positives the model predicted out of all the instances it predicted as positive. A precision score of 0.81 means that out of all the instances that the model predicted as positive, only 81% of them were actually positive.\n",
    "3. <b>Recall</b> is a measure of how many of the actual positive instances the model correctly identified. In other words, it measures how many true positives the model predicted out of all the actual positive instances. A recall score of 0.89 means that the model correctly identified 89% of all the actual positive instances.\n",
    "4. The <b>F1 Score</b> of 0.84 is a harmonic mean of precision and recall, which is a useful metric when there is an imbalance in the number of instances of each class. In this case, the F1 score indicates a good balance between precision and recall.\n",
    "5. Our <b>Confusion Matrix</b> is a visualisation of the distribution of correct and incorrect predictions across the two classes in table format. In this case, the model made 16843 true negative predictions, 1405 false negative predictions, 747 false positive predictions, and 5830 true positive predictions. The large number of true negatives and true positives indicates that the model is doing well in correctly identifying both negative and positive instances. However, the relatively high number of false negatives and false positives suggests that there is still room for improvement in the model. <b>In particular, we should pay attention to the False Negative value, as it is of paramount importance that we reduce the number of misdiagnoses for those who are vulnerable to COVID19</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d71695",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest Classifier Classification Report:\\n\", classification_report(random_forest_y_train, random_forest_y_train_pred_class))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd4ad8ee",
   "metadata": {},
   "source": [
    "From the classification report, we can see that our model is more comfortable in predicting the negative class with a precision of 0.96 and a recall of 0.92. This means that among all the instances that the model predicted as negative, 96% of them were actually negative and the model correctly identified 92% of all actual negative instances.\n",
    "\n",
    "However, our random forest classifier  model is less comfortable in predicting the positive class (class 1) with a precision of 0.81 and a recall of 0.89. Only 81% of them were actually positive and the model correctly identified 89% of all actual positive instances. \n",
    "\n",
    "In comparison with the other models, despite the random forest classifier's higher overall accuracy, it has a lower accuracy than linear regression and logistic regression when attempting to predict the positive class. As this is our priority we should always bear this in mind when evalutating the performance of our models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24ddbc28",
   "metadata": {},
   "source": [
    "## 4.4.1: Classification Evaluation Comparison Between Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31402dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict target feature values for full testing set\n",
    "random_forest_y_test_pred = rfc.predict_proba(random_forest_x_test)[:, 1]\n",
    "\n",
    "# threshold predicted target feature values at 0.5 to get predicted class for full testing set\n",
    "random_forest_y_test_pred_class = (random_forest_y_test_pred >= 0.5).astype(int)\n",
    "\n",
    "random_forest_accuracy_test = accuracy_score(random_forest_y_test, random_forest_y_test_pred_class)\n",
    "random_forest_precision_test = precision_score(random_forest_y_test, random_forest_y_test_pred_class)\n",
    "random_forest_recall_test = recall_score(random_forest_y_test, random_forest_y_test_pred_class)\n",
    "random_forest_f1_test = f1_score(random_forest_y_test, random_forest_y_test_pred_class)\n",
    "\n",
    "random_forest_confusion_test = confusion_matrix(random_forest_y_test, random_forest_y_test_pred_class)\n",
    "\n",
    "random_forest_mae_test = mean_absolute_error(random_forest_y_test, random_forest_y_test_pred_class)\n",
    "random_forest_mse_test = mean_squared_error(random_forest_y_test, random_forest_y_test_pred_class)\n",
    "random_forest_rmse_test = np.sqrt(random_forest_mse_test)\n",
    "random_forest_r2_test = r2_score(random_forest_y_test, random_forest_y_test_pred_class)\n",
    "\n",
    "print(\"\\nEvaluation metrics on the testing set:\")\n",
    "print()\n",
    "print(\"Accuracy: {:.2f}\".format(random_forest_accuracy_test))\n",
    "print(\"Precision: {:.2f}\".format(random_forest_precision_test))\n",
    "print(\"Recall: {:.2f}\".format(random_forest_recall_test))\n",
    "print(\"F1 score: {:.2f}\".format(random_forest_f1_test))\n",
    "print()\n",
    "print(\"Confusion matrix:\\n\", random_forest_confusion_test)\n",
    "print()\n",
    "print(\"Mean Squared Error: {:.2f}\".format(random_forest_mse_test))\n",
    "print(\"Mean Absolute Error: {:.2f}\".format(random_forest_mae_test))\n",
    "print(\"R2 score: {:.2f}\".format(random_forest_r2_test))\n",
    "print(\"Root Mean Squared Error: {:.2f}\".format(random_forest_rmse_test))\n",
    "\n",
    "list_of_random_forest_testing_stats = [random_forest_accuracy_test, random_forest_precision_test, random_forest_recall_test, random_forest_mse_test, random_forest_mae_test, random_forest_rmse_test, random_forest_r2_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5663d5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report:\\n\", classification_report(random_forest_y_train, random_forest_y_train_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc106bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['Accuracy', 'Precision', 'Recall', 'MSE', 'MAE', 'RMSE', 'R2']\n",
    "\n",
    "for i in range(len(metrics)):\n",
    "    print(metrics[i])\n",
    "    print('='*30)\n",
    "    print('Trained Data: ', list_of_random_forest_training_stats[i])\n",
    "    print('Tested Data: ', list_of_random_forest_testing_stats[i])\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36a47590",
   "metadata": {},
   "source": [
    "The performance of the random forest model between training and test data is, similar to the previous two models, quite high. Accuracy, recall and F1 score remain consistent, while precision is different by 0.02. Performance on the confusion matrices are, again, very similar too. So far,the random forst model seems to be the most accurate model by a very small margin, with a performance increase of .01 over the logistic regression model and .02 over the linear regression model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "595b73bc",
   "metadata": {},
   "source": [
    "## 4.4.2 K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b357cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing 5 fold K-fold cross validation on random forest model using full dataset\n",
    "\n",
    "#concatenating train and test datasets for independent and dependent variables\n",
    "X = pd.concat([train[['case_month', 'age_group', 'hosp_yn', 'icu_yn', 'underlying_conditions_yn']], \n",
    "                                 test[['case_month', 'age_group', 'hosp_yn', 'icu_yn', 'underlying_conditions_yn']]])\n",
    "y = pd.concat([train['death_yn'], test['death_yn']]) \n",
    "\n",
    "#folds for cross validation\n",
    "folds = 5\n",
    "\n",
    "#creating K-Fold object\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "\n",
    "#K-fold cross validation scores\n",
    "mse_scores = -cross_val_score(rfc, X, y, cv=folds, scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "#printing results\n",
    "print(f\"MSE scores: {mse_scores}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95df6de6",
   "metadata": {},
   "source": [
    "Again ,the MSE scores produced by the K-Fold cross validation process performed on the randon forest model are quite low, with none exceeding .097. This indicates that the random forest implementation is relatively sucessful and predicting the dependent outcome based on the provided independent features, even when performed on subsets of the full dataset. This is consistent with the relatively high accuracy yielded by the model when performing analysis of the testing and training data. Taken together, the model performance information and the K-fold cross validation information give us a good indication that this model would likely perform well on new data. \n",
    "\n",
    "In terms of comparison to the other models, the random forest model performed slightly better on the cross validation than the logistic regression model, and slightly worse than the linear regression model. Due to the closeness in performance to the other models, and the higher overall accuracy of the random forest model, the best overall model for this predictive problem is likely debateable, with arguments for the suitability of each of the three."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "986cf1b1",
   "metadata": {},
   "source": [
    "# Part 5\n",
    "\n",
    "## 5.1.1 Comparing the Performance of Each of our Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dccea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['Accuracy', 'Precision', 'Recall', 'MSE', 'MAE', 'RMSE', 'R2']\n",
    "models = ['Linear Regression', 'Logistic Regression', 'Random Forest']\n",
    "num_models = len(models)\n",
    "\n",
    "data = np.array([list_of_linear_regression_training_stats, \n",
    "                 list_of_logistic_regression_training_stats, \n",
    "                 list_of_random_forest_training_stats])\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(metrics), ncols=1, figsize=(20, 6 * len(metrics)))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    for model in range(num_models):\n",
    "        ax.bar(models[model], data[model, i], width=0.3, color=['#8FBBAF', '#C06C84', '#6C557B'][model])\n",
    "    fig.set_facecolor('#f5f5f5')    \n",
    "    ax.set_title(metric, fontsize=20)\n",
    "\n",
    "plt.subplots_adjust(hspace=1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd452de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['Accuracy', 'Precision', 'Recall', 'MSE', 'MAE', 'RMSE', 'R2']\n",
    "models = ['Linear Regression', 'Logistic Regression', 'Random Forest']\n",
    "\n",
    "data = np.array([list_of_linear_regression_training_stats, \n",
    "                 list_of_logistic_regression_training_stats, \n",
    "                 list_of_random_forest_training_stats])\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    print(f\"\\n{metric}:\")\n",
    "    for j, model in enumerate(models):\n",
    "        value = data[j, i]\n",
    "        print(f\"{model}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7af299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbc60365",
   "metadata": {},
   "source": [
    "## 5.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d04bf99",
   "metadata": {},
   "source": [
    "### How to improve: \n",
    "\n",
    "paragraph on each of the below\n",
    "\n",
    "1. Feature Selection\n",
    "2. Creating New Features\n",
    "3. Combining Predictive Models\n",
    "4. Revised Splitting protocol\n",
    "5. Feature Rescaling\n",
    "6. Feature Ranking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
